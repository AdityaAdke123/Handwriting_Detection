{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV,cross_val_score, cross_val_predict\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, mean_squared_error, precision_recall_curve, roc_curve, r2_score, ConfusionMatrixDisplay, auc, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import stats\n",
    "from scipy.stats import expon, randint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adress elements of a list:\n",
    "# my_list[0], my_list[1]\n",
    "\n",
    "# adress  a column in a numpy array:\n",
    "# arr[:,2]  to adress more columns arr[:,[1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import the data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset form CVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./MarathonData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Librarie\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer(return_X_y=False, as_frame=True)\n",
    "# as_frame=True: pandas data frame\n",
    "# as_frame=Fale: no data fram \n",
    "X = data.data\n",
    "t = data.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Features: \n",
    "df = df.drop(columns=['Name', 'Marathon','id'])\n",
    "\n",
    "# Drop Samples:\n",
    "df = df.dropna(subset=['Category'])\n",
    "\n",
    "# Convert to numeric values:\n",
    "df['Wall21'] = pd.to_numeric(df['Wall21'])\n",
    "\n",
    "# Impute Missing Values \n",
    "df['CrossTraining'] = df['CrossTraining'].fillna('None')\n",
    "\n",
    "# adding new features \n",
    "#df['NewFeature'] = df[]/df['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson's Correlation Coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the data and the target a seperate:\n",
    "#df = pd.concat([data,target], axis 1)\n",
    "\n",
    "# get the pearons correlation coefficient \n",
    "corr_matrix = df.corr(method='pearson', numeric_only=True)\n",
    "\n",
    "# get the largest correlated feature \n",
    "target = 'MarathonTime'\n",
    "best_feature = corr_matrix[target].drop(target).idxmax()\n",
    "print(f\"Feature with the largest predictive value for Target is: '{best_feature}'\")\n",
    "\n",
    "# get the value of the highest corrleted feature \n",
    "highest_cor_feat = corr_matrix[target].drop(target).max()\n",
    "print(f\"The Pearson Coefficent Value is: {highest_cor_feat:.2f} \")\n",
    "\n",
    "# print the Correlation Values by order \n",
    "corr_matrix[target].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Correlation Matrix using a Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Splitting Data in Training and Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET AN OVERVIEW OF THE DISTRBUTION HOW MANY UNIQUE TIMES ARE IN A PANDAS COLUMNY\n",
    "t.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Distribution of the Target:   Simple 80/20 split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE: If the data is seperated in Data X and Target y\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, \n",
    "                                                    test_size =0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True)\n",
    "                                                    \n",
    "\n",
    "# PIPELINE: If the data is one dataset and get seperated later \n",
    "test_set,train_set = train_test_split(df,\n",
    "                                  test_size=0.2,\n",
    "                                  random_state=42,\n",
    "                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target is imbalanced and binary: Stratified 80/20 split based on target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE: If the data is seperated in Data X and Target y\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, \n",
    "                                                    test_size =0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True, \n",
    "                                                    stratify=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for testings\n",
    "X_train_c, X_test_c, t_train_c, t_test_c = train_test_split(X, t, \n",
    "                                                    test_size =0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True, \n",
    "                                                    stratify=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most predictive Feature is Imbalanced and continuous: Categories Feature into bins perform a stratified split based on the bins "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process of partitioning the dataset into subsets while preserving the (categorical) classes prior probabilities the same as in the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get of how the bins should look like:\n",
    "# About 95% of the samples lie in the interval [a,b]\n",
    "best_feature= 'Wall21'\n",
    "a = df[best_feature].mean() - 1.96*df[best_feature].std(ddof=1)\n",
    "b = df[best_feature].mean() + 1.96*df[best_feature].std(ddof=1)\n",
    "print(f\"About 95% of the samples lie in the interval [{a:.1f},{b:.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[best_feature].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature = 'Wall21'\n",
    "\n",
    "# Categorize feature one in order to performed stratified splitting\n",
    "cat = pd.cut(df[best_feature],\n",
    "                bins=[0.,1.5, 3.0, 4.5, 6., np.inf],\n",
    "                labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "\n",
    "# Pipeline to perform a stratified split \n",
    "train_set, test_set, cat_train, cat_test = train_test_split(df, cat, \n",
    "                                                        test_size=0.2,\n",
    "                                                        shuffle=True,\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=cat)\n",
    "\n",
    "plt.hist(cat_train, color='blue', label='Train Set', alpha=0.3)\n",
    "plt.hist(cat_test, color='orange', label='Test Set', alpha=0.3)\n",
    "plt.legend();\n",
    "\n",
    "# TRAINING\n",
    "\n",
    "# target values \n",
    "t_train = train_set['MarathonTime'].copy()\n",
    "\n",
    "# feature matrix \n",
    "X_train = train_set.drop(labels='MarathonTime',axis=1)\n",
    "\n",
    "# TESTING\n",
    "\n",
    "# target values \n",
    "t_test = test_set['MarathonTime'].copy()\n",
    "\n",
    "# feature matrix \n",
    "X_test = test_set.drop(labels='MarathonTime', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Pipline for encoding Numericial and Categorical Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = ['km4week', 'sp4week','Wall21']\n",
    "cat_attribs = ['Category', 'CrossTraining']\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer([('num', StandardScaler(), num_attribs),\n",
    "                                           ('cat', OneHotEncoder(), cat_attribs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate Pipelines for Encoding Numeric and Categorical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMERIC ATTRIBUTES\n",
    "num_attribs = ['km4week', 'sp4week','Wall21']\n",
    "\n",
    "num_pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), \n",
    "                       ('scaler', StandardScaler())                 \n",
    "])\n",
    "\n",
    "# CATEGORIAL PIPELINE \n",
    "cat_attribs = ['Category', 'CrossTraining']\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[  \n",
    "   ('encoder', OneHotEncoder(handle_unknown='ignore'))  \n",
    "])\n",
    "\n",
    "# COMBINE CATEGORIAL AND NUMERICAL PIPELINES\n",
    "preprocessing_pipeline = ColumnTransformer(transformers=[\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', cat_pipeline, cat_attribs)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run just the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN PIPELINES\n",
    "X_train_prepared = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test_prepared = preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST FOR VISUALIZATION:\n",
    "\n",
    "# STACK all names of Categorial and Numerical Attributes togetter \n",
    "attribute_labels = np.hstack((\n",
    "    num_attribs,  # Numerical Attributes\n",
    "    #*preprocessing_pipeline.named_transformers_['cat'].categories_  # Categorical Attributes\n",
    "    *preprocessing_pipeline.named_transformers_['cat']['encoder'].categories_  # Try this if a seperate Pipeline for the categoris is used\n",
    "))\n",
    "\n",
    "# Visualize the data frame\n",
    "data_prepared = pd.DataFrame(X_train_prepared, \n",
    "                                   columns=attribute_labels,\n",
    "                                   index=X_train.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECSION TREE CLASSIFIER \n",
    "pipeline = Pipeline(steps=[('preprocessing', preprocessing_pipeline),\n",
    "                           ('tree', DecisionTreeClassifier(random_state=0))])\n",
    "\n",
    "\n",
    "# RANDOM FOREST REGRESSOR !!!!!!!!!!!!!!! Watch out Regressor and Classifier!\n",
    "pipeline = Pipeline(steps=[('preprocessing', preprocessing_pipeline),\n",
    "                           ('tree', RandomForestRegressor())])\n",
    "\n",
    "# LOGISTIC REGRESSION WITH LASSO REGULIZER (L1 PENALITY)\n",
    "pipeline = Pipeline(steps=[('preprocessing',preprocessing_pipeline),\n",
    "                           ('log_reg', LogisticRegression(penalty='l1', solver='saga'))])\n",
    "\n",
    "\n",
    "# LASSO MODEL WITH POLYNOMIAL FEATURES \n",
    "pipeline = Pipeline(steps=[('preprocessing', preprocessing_pipeline),\n",
    "                       ('poly', PolynomialFeatures()),\n",
    "                       ('Lasso',Lasso(max_iter=1000))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just use on Pipeline for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF ALL INPFEATURES ARE NUMERIC\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('lasso', Lasso(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIF FEAUTRES ARE NUMERIC ANS CATEGORICAL \n",
    "num_attribs = ['km4week', 'sp4week', 'Wall21']\n",
    "cat_attribs = ['Category', 'CrossTraining']\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', ColumnTransformer([\n",
    "        ('num', StandardScaler(), num_attribs),\n",
    "        ('cat', OneHotEncoder(), cat_attribs)\n",
    "    ])),\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('lasso', Lasso(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for Testing\n",
    "pipeline = Pipeline([('scaler', MinMaxScaler()),\n",
    "                     ('log_reg', LogisticRegression(penalty='l1',max_iter=200, solver='saga'))])\n",
    "\n",
    "param_grid = {\n",
    "    'log_reg__C': [0.01, 0,1,1,2,3,4,5,6,7,8,9,10,20,30,40,50,100]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=0),\n",
    "    scoring='neg_mean_squared_error',\n",
    "    refit=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_c,t_train_c)\n",
    "best_model_c = grid_search.best_estimator_\n",
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use get params first to get all the tunable hyperparamters. \n",
    "\n",
    "# SET OPTIMAZATION PARAMETERS (DICTIONARY)\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "param_grid = {\n",
    "    'log_reg__C': np.linspace(0.01,10,100)\n",
    "}\n",
    "\n",
    "# LINEAR REGRESSION WITH LASSO REGULIZER\n",
    "param_grid = {'poly__degree': list(range(3,9)), # just for poly lasso model \n",
    "              'Lasso__alpha': [1e-5, 1e-4, 1e-3, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "# DECISION TREE\n",
    "param_grid = {\n",
    "    'tree__criterion': ['gini', 'entropy'],\n",
    "    'tree__max_depth': np.arange(1,50),\n",
    "    'tree__min_samples_split': np.arange(2,10),\n",
    "    'tree__min_samples_leaf': np.arange(1,10)\n",
    "}\n",
    "\n",
    "# RANDOM FOREST\n",
    "param_grid = {\n",
    "    'tree__n_estimators': list(range(50,400)),\n",
    "    'tree__criterion': ['gini', 'entropy'],\n",
    "    'tree__max_depth': [5,6,7]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE GRIDSEARCH OPTIMAZAION \n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=0), # cv scheme\n",
    "    scoring='neg_mean_squared_error',\n",
    "    # scoring='f1_weighted'\n",
    "    # scoring = 'accuracy'\n",
    "    # scoring = 'r2'\n",
    "    refit=True,\n",
    "    n_jobs=-1  # Parallel processing\n",
    ")\n",
    "\n",
    "# FITTING THE MODEL \n",
    "grid_search.fit(X_train,t_train)\n",
    "\n",
    "# USE THE BEST PARAMS FOR THE MODEL \n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# PRINT THE BEST PARAMS \n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE RANDOMIZED SEARCH OPTIMAZATION\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,  \n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=0), # CV scheme\n",
    "    scoring='neg_mean_squared_error',\n",
    "    # scoring = 'accuracy'\n",
    "    # scoring = 'r2\n",
    "    refit=True,\n",
    "    n_jobs=-1  # Parallel processing\n",
    ")\n",
    "\n",
    "# FITTING THE MODEL \n",
    "random_search.fit(X_train,t_train)\n",
    "\n",
    "# USE THE BEST PARAMS FOR THE MODEL \n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# PRINT THE BEST PARAMS \n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = best_model.predict(X_train)\n",
    "y_test = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95 Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL FUNCTION TO PRINT CONFIDENCE INTERVALLS\n",
    "scores_val = cross_val_score(best_model, X_train, t_train,\n",
    "                            scoring='f1_weighted', \n",
    "                             cv=KFold(n_splits=5, shuffle=True, random_state=0))\n",
    "\n",
    "confidence = 0.95\n",
    "stats.t.interval(confidence,\n",
    "                 len(scores_val)-1,\n",
    "                 loc = scores_val.mean(),\n",
    "                 scale=scores_val.std(ddof=1)/np.sqrt(len(scores_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2 Score & 95CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE THE R2 FOR TRAIN AND TEST SET \n",
    "r2_train = r2_score(t_train, y_train)\n",
    "r2_test = r2_score(t_test, y_test)\n",
    "\n",
    "\n",
    "# CROSS VALIDATION\n",
    "r2_scores = cross_val_score(best_model,\n",
    "                            X_train, \n",
    "                            t_train, \n",
    "                            scoring='r2', \n",
    "                            cv=10) \n",
    "\n",
    "\n",
    "# 95% CONFIDENCE INTERVALL FOR R2 SCORES\n",
    "mean_r2 = r2_scores.mean()\n",
    "std_r2 = r2_scores.std()\n",
    "n = len(r2_scores)  # number of folds (should be > 5)\n",
    "\n",
    "# 95% CONFIDENCE INTERVAL\n",
    "confidence_interval = stats.t.interval(0.95, n-1, loc=mean_r2, scale=std_r2/np.sqrt(n))\n",
    "\n",
    "\n",
    "# PRINT RESULT\n",
    "print('model_name','\\n---------------------------------')\n",
    "print('R2 Train: ', r2_train)\n",
    "print('R2 Test: ', r2_test)\n",
    "print('Mean:', r2_scores.mean())\n",
    "print('Standard deviation:', r2_scores.std())\n",
    "print(f'95% confidence interval for R2: ({confidence_interval[0]:.3f}, {confidence_interval[1]:.3f})\\n')\n",
    "\n",
    "# PRINT RESULT OF SCORES\n",
    "print('\\nScores:', r2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE & CI95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET CONFIDENCE INTERVAL\n",
    "confidence = 0.95\n",
    "\n",
    "# TRAIN SET\n",
    "squared_errors_train = (t_train - y_train)**2\n",
    "a,b = stats.t.interval(confidence,\n",
    "                       len(squared_errors_train)-1,\n",
    "                       loc = squared_errors_train.mean(),\n",
    "                       scale=squared_errors_train.std(ddof=1)/np.sqrt(len(squared_errors_train)))\n",
    "\n",
    "# TEST SET\n",
    "squared_errors_test = (t_test - y_test)**2\n",
    "c,d=stats.t.interval(confidence,\n",
    "                     len(squared_errors_test)-1,\n",
    "                     loc = squared_errors_test.mean(),\n",
    "                     scale=squared_errors_test.std(ddof=1)/np.sqrt(len(squared_errors_test)))\n",
    "    \n",
    "# PRINT RESULT\n",
    "print('model_name','\\n---------------------------------')\n",
    "print('RMSE Train: ', np.sqrt(mean_squared_error(t_train, y_train)))\n",
    "print(confidence*100,'% CI Train = [',np.sqrt(np.max([0,a])),',',np.sqrt(b),']')\n",
    "    \n",
    "print('\\nRMSE Test: ', np.sqrt(mean_squared_error(t_test, y_test)))\n",
    "print(confidence*100,'% CI Test = [',np.sqrt(np.max([0,c])),',',np.sqrt(d),']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model.named_steps['Lasso'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.hstack((best_model.named_steps['Lasso'].intercept_,\n",
    "              best_model.named_steps['Lasso'].coef_))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.stem(w)\n",
    "\n",
    "\n",
    "#plt.ylabel('Weight values', size=15)\n",
    "#plt.xticks(np.arange(len(w)), ['$w_{'+str(i)+'}$' for i in range(len(w))],rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Prediction vs true Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xline = np.linspace(0,1300,1301)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(t_train, y_train); plt.plot(xline, xline, 'r')\n",
    "plt.title('Training Set'); plt.xlabel('True Target'); plt.ylabel('Predicted Target')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(t_test, y_test); plt.plot(xline, xline, 'r')\n",
    "plt.title('Test Set'); plt.xlabel('True Target'); plt.ylabel('Predicted Target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ROC/AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multiclass prediction\n",
    "\n",
    "AUCs_ROC = []\n",
    "for i in range(7):\n",
    "    fpr, tpr, thresholds = roc_curve(1*(t_test==i), 1*(y_test==i))\n",
    "    AUCs_ROC += [auc(fpr, tpr)]\n",
    "    plt.plot(fpr, tpr, label='Class '+str(i))\n",
    "\n",
    "plt.legend(fontsize=15); plt.xlabel('FPR', size=15); plt.ylabel('TPR', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC/AUC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(t_test,best_model.predict_proba(X_test)[:,1])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                 estimator_name='Logistics-basic')\n",
    "display.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiclass predictions\n",
    "y_scores = best_model.decision_function(X_test)\n",
    "\n",
    "AUCs = []\n",
    "for i in range(7):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(1*(t_test==i), y_scores[:,i])\n",
    "    AUCs += [auc(recalls, precisions)]\n",
    "    plt.plot(recalls, precisions, label='Class '+str(i))\n",
    "\n",
    "plt.legend(fontsize=15); plt.xlabel('Recall', size=15); plt.ylabel('Precision', size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single class predictions using cross-validation\n",
    "z_train = cross_val_predict(pipeline, X_train, t_train, \n",
    "                             cv=10, method='decision_function')\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(t_train, z_train)\n",
    "\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall', size=15)\n",
    "plt.ylabel('Precision', size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold for a Precision >= 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = np.where(precision >= 0.90)[0]\n",
    "idx_optimal = np.argmax(recall[idx])\n",
    "print('precision:',precision[idx[idx_optimal]])\n",
    "print('recall:',recall[idx[idx_optimal]])\n",
    "print('thresholds',thresholds[idx[idx_optimal]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall, F1, Accuray and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SET \n",
    "print('Training set performance:')\n",
    "\n",
    "# ACCURACY AND CLASSIFICATION REPORT\n",
    "accuracy = metrics.accuracy_score(t_train, y_train)\n",
    "print(\"Accuracy: %.3f\" % accuracy)\n",
    "print(classification_report(t_train, y_train))\n",
    "\n",
    "# CONFUION MATRIX FOR THE TEST SET\n",
    "accuracy = metrics.accuracy_score(t_train, y_train)\n",
    "print(\"Accuracy: %.3f\" % accuracy)\n",
    "\n",
    "cm = metrics.confusion_matrix(t_train, y_train)\n",
    "sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "\n",
    "# TEST SET \n",
    "print('Test set performance:')\n",
    "\n",
    "# ACCURACY AND CLASSIFICATION REPORT\n",
    "accuracy = metrics.accuracy_score(t_test, y_test)\n",
    "print(\"Accuracy: %.3f\" % accuracy)\n",
    "print(classification_report(t_test, y_test))\n",
    "\n",
    "# CONFUION MATRIX FOR THE TEST SET\n",
    "accuracy = metrics.accuracy_score(t_test, y_test)\n",
    "print(\"Accuracy: %.3f\" % accuracy)\n",
    "\n",
    "cm = metrics.confusion_matrix(t_test, y_test)\n",
    "sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualize which vectors are set to zero (just for Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest:\n",
    "np.where(best_model.named_steps['Lasso'].coef_[0,:]!=0)[0].shape # erstes Target Feture\n",
    "np.where(best_model.named_steps['Lasso'].coef_[1,:]!=0)[0].shape # zweites Target Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STACK all names of Categorial and Numerical Attributes together\n",
    "attribute_labels = np.hstack((\n",
    "    num_attribs,  # Numerical Attributes\n",
    "    *preprocessing_pipeline.named_transformers_['cat']['encoder'].categories_  # Categorical Attributes\n",
    "))\n",
    "\n",
    "# Access the coefficients from the best model\n",
    "lasso_coefs = best_model.named_steps['Lasso'].coef_\n",
    "\n",
    "# Get excluded and included features without creating a DataFrame\n",
    "excluded_features = attribute_labels[lasso_coefs == 0]\n",
    "print(f\"Excluded features in Lasso:\\n {excluded_features}\")\n",
    "\n",
    "included_features = attribute_labels[lasso_coefs != 0]\n",
    "print(f\"\\nIncluded features in Lasso:\\n {included_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STACK all names of Categorial and Numerical Attributes togetter \n",
    "attribute_labels = np.hstack((\n",
    "    num_attribs,  # Numerical Attributes\n",
    "    #*preprocessing_pipeline.named_transformers_['cat'].categories_  # Categorical Attributes\n",
    "    *preprocessing_pipeline.named_transformers_['cat']['encoder'].categories_  # Try this if a seperate Pipeline for the categoris is used\n",
    "))\n",
    "\n",
    "# Visualize the data frame\n",
    "data_prepared = pd.DataFrame(X_train_prepared, \n",
    "                                   columns=attribute_labels,\n",
    "                                   index=X_train.index)\n",
    "\n",
    "# access the coefficients\n",
    "lasso_coefs = best_model.named_steps['Lasso'].coef_\n",
    "\n",
    "# Get included features\n",
    "excluded_features = np.array(data_prepared.columns)[lasso_coefs == 0]\n",
    "print(f\"Excluded features in Lasso:\\n {excluded_features}\")\n",
    "\n",
    "# Get the included features\n",
    "included_features = np.array(prepared.columns)[lasso_coefs != 0]\n",
    "print(f\"\\nIncluded features in Lasso:\\n {included_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the final weight coefficients (end of lecture 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access coefficients for the linear regression model\n",
    "linreg_coefs = best_model.named_steps['Lasso'].coef_ # acces the coeeficeints \n",
    "lin_reg_intersept = best_model.named_steps['Lasso'].intercept_ # acces the intercepts, look at the program down below to how to stack them \n",
    "\n",
    "# Get the feature names after preprocessing\n",
    "feature_names = preprocessing_pipeline.get_feature_names_out()\n",
    "\n",
    "# Combine feature names with coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': linreg_coefs\n",
    "})\n",
    "\n",
    "# Sort by the absolute value of the coefficients to see the most impactful features\n",
    "coef_df['Abs_Coefficient'] = coef_df['Coefficient'].abs()\n",
    "coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "coef_df[['Feature', 'Coefficient']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lasso_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# concatenating all parameters into vector w\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((\u001b[43mlasso_model\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlasso\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mintercept_, \n\u001b[1;32m      4\u001b[0m                lasso_model\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlasso\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcoef_[:,np\u001b[38;5;241m.\u001b[39mnewaxis]))\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mstem(w)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lasso_model' is not defined"
     ]
    }
   ],
   "source": [
    "# concatenating all parameters into vector w (end of lecture 6)\n",
    "\n",
    "w = np.vstack((best_model.named_steps['lasso'].intercept_, # the intercept is the w0 parameter (in y = ax + b, b is the intercept)\n",
    "               best_model.named_steps['lasso'].coef_[:,np.newaxis])) # acces all the coefficients w1 to wN, a are the coeffiencts \n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.stem(w)\n",
    "plt.ylabel('Weight values', size=15)\n",
    "plt.xticks(np.arange(len(w)), ['$w_{'+str(i)+'}$' for i in range(len(w))],rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "R² Score: Measures how well the model explains the variance of the target.\n",
    "Precision: Measures the accuracy of positive predictions.\n",
    "F1 Score: Balances precision and recall to provide a single metric.\n",
    "The 95% Confidence Interval around each of these metrics provides a range where the true metric value is likely to lie, offering insight into the reliability of the metric estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. R² Score (Coefficient of Determination)\n",
    "Definition: The R² score measures the proportion of the variance in the dependent variable (target) that is predictable from the independent variables (features). It ranges from 0 to 1, where 1 indicates perfect prediction, and 0 means the model doesn't explain any of the variance.\n",
    "Interpretation:\n",
    "R² = 0: The model does not explain the variability of the data at all.\n",
    "R² = 1: The model perfectly predicts the outcome.\n",
    "Negative R²: The model performs worse than a horizontal line (i.e., the model is worse than predicting the mean of the data).\n",
    "95% Confidence Interval (CI): The CI for the R² score gives a range of values within which the true R² score lies with 95% confidence. A narrow interval suggests the R² score is reliable, while a wide interval implies greater uncertainty about the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Precision (Positive Predictive Value)\n",
    "Definition: Precision is the ratio of true positives (correct positive predictions) to the total number of predicted positives (true positives + false positives). It measures the accuracy of the positive predictions.\n",
    "Precision = TP/(TP + FP)\n",
    "\n",
    " \n",
    "Interpretation:\n",
    "High precision: Indicates that most of the predicted positives are indeed correct (few false positives).\n",
    "Low precision: Many of the predicted positives are incorrect (high false positive rate).\n",
    "95% Confidence Interval: The CI for precision estimates the range in which the true precision lies with 95% confidence. It reflects the model’s ability to avoid false positives, and a narrow interval suggests a reliable precision estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " F1 Score\n",
    "Definition: The F1 score is the harmonic mean of precision and recall, and it balances the two metrics. It’s particularly useful when you need a balance between precision and recall (especially in imbalanced datasets)\n",
    "​Precision focuses on minimizing false positives, while recall focuses on minimizing false negatives, and F1 balances both.\n",
    "Interpretation:\n",
    "High F1 score: The model has both high precision and high recall, meaning it is good at both predicting positive cases and avoiding false negatives.\n",
    "Low F1 score: The model may be either poor at precision or recall, meaning it either predicts too many false positives or misses a lot of true positives.\n",
    "95% Confidence Interval: The CI for the F1 score provides a range in which the true F1 score is likely to fall, with 95% confidence. A narrow interval means the F1 score estimate is more reliable, while a wide interval indicates uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error (RMSE)\n",
    "Definition: RMSE is a standard way to measure the error of a model in predicting continuous outcomes. It represents the square root of the average of the squared differences between predicted and actual values. The formula is:\n",
    "\n",
    "Interpretation:\n",
    "Low RMSE: Indicates that the predicted values are close to the actual values, meaning the model performs well.\n",
    "High RMSE: Indicates that the model’s predictions are, on average, far from the actual values, meaning the model has poor performance.\n",
    "RMSE has the same units as the dependent variable, making it easily interpretable as an average error in the same units as the predicted outcome.\n",
    "95% Confidence Interval: The confidence interval for RMSE provides a range in which the true RMSE is likely to fall with 95% confidence. A narrow CI indicates the model's error estimate is reliable and consistent, while a wide CI suggests variability or uncertainty in the model’s performance. The CI can be calculated through bootstrapping or other statistical techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
